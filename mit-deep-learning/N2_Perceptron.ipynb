{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    A simple Perceptron implementation.\n",
    "\n",
    "    The Perceptron is a fundamental binary classifier that uses a linear decision boundary \n",
    "    to classify data points. This implementation includes basic functionality of a Perceptron \n",
    "    with customizable activation functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dimension_data : int\n",
    "        The number of features (excluding the bias) in the input data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : callable\n",
    "        The function used for activation.\n",
    "    diff_activation_function : callable\n",
    "        The derivative of the activation function.\n",
    "    bias : float\n",
    "        The bias term added to the input data.\n",
    "    weights : np.ndarray\n",
    "        The weights of the perceptron, initialized to None.\n",
    "    activation_function_name : str\n",
    "        The name of the activation function currently in use.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    set_activation_function(name='sigmoid')\n",
    "        Set the activation function by name.\n",
    "    insert_bias(x)\n",
    "        Insert a bias term into the input vector.\n",
    "    loss_function(weights, x, y)\n",
    "        Calculate the loss function given weights, inputs, and the target.\n",
    "    compute_loss(predictions, targets)\n",
    "        Compute the loss between predictions and targets.\n",
    "    forward(X, y, epochs=100, learning_rate=0.01)\n",
    "        Run the forward training loop for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension_data) -> None:\n",
    "        self.activation_function = None\n",
    "        self.diff_activation_function = None\n",
    "        self.bias = 1\n",
    "        self.weights = None\n",
    "        self.activation_function_name = 'sigmoid'\n",
    "        self.average_loss = []\n",
    "    \n",
    "    def set_activation_function(self, name = 'sigmoid'):\n",
    "        \"\"\"\n",
    "        Set the activation function for the perceptron.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str, optional\n",
    "            The name of the activation function. Supported values are 'relu', 'sigmoid', and 'tanh'.\n",
    "            Default is 'sigmoid'.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the specified activation function name is not supported.\n",
    "        \"\"\"\n",
    "        self.activation_function_name = name.lower()\n",
    "        if name.lower() == 'relu':\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "            self.diff_activation_function = lambda x: np.where(x > 0, 1, 0)\n",
    "        elif name.lower() == 'sigmoid':\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.diff_activation_function = lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif name.lower() == 'tanh':\n",
    "            self.activation_function = lambda x: np.tanh(x)\n",
    "            self.diff_activation_function = lambda x: 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "    def insert_bias(self, x):\n",
    "        return np.insert(x, 0, self.bias)  # Always insert bias at index 0\n",
    "    \n",
    "    def loss_function(self, weights, x, y):\n",
    "        prediction = self.activation_function(np.dot(weights, x))\n",
    "        return prediction - y\n",
    "\n",
    "    def compute_loss(self, predictions, targets):\n",
    "        # Check which activation function is used based on a stored name attribute\n",
    "        if self.activation_function_name == 'sigmoid':\n",
    "            # Binary cross-entropy for sigmoid activation\n",
    "            return -np.mean(targets * np.log(predictions + 1e-9) + (1 - targets) * np.log(1 - predictions + 1e-9))\n",
    "        elif self.activation_function_name == 'relu' or self.activation_function_name == 'tanh':\n",
    "            # Mean squared error for ReLU or tanh activations in a regression context\n",
    "            return np.mean((predictions - targets) ** 2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported or undefined activation function name: {self.activation_function_name}\")\n",
    "\n",
    "    def backward(self, X_i, error, z, learning_rate):\n",
    "        \"\"\"\n",
    "        Perform the backward pass, updating the weights based on the error and gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_i : np.ndarray\n",
    "            The input data for a single instance, including the bias.\n",
    "        error : float\n",
    "            The difference between the predicted and actual target.\n",
    "        z : float\n",
    "            The linear combination of weights and inputs before activation.\n",
    "        learning_rate : float\n",
    "            The rate at which the weights should be updated.\n",
    "        \"\"\"\n",
    "        gradient = error * self.diff_activation_function(z) * X_i  # Compute gradient\n",
    "        self.weights -= learning_rate * gradient  # Update weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass by computing the output of the perceptron.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The input data array where each row represents an instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The predictions computed by the network.\n",
    "        \"\"\"\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Insert bias\n",
    "        z = np.dot(X, self.weights)  # Compute the linear combination\n",
    "        predictions = self.activation_function(z)  # Apply activation function\n",
    "        return predictions\n",
    "\n",
    "                \n",
    "    def train(self, X, y, epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        The perceptron Supervised Training using a provided dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The input data matrix where each row represents an instance and each column a feature.\n",
    "        y : np.ndarray\n",
    "            The target output vector where each element corresponds to a target for a corresponding row in X.\n",
    "        epochs : int, optional\n",
    "            The number of epochs for which the model should be trained. Defaults to 100.\n",
    "        learning_rate : float, optional\n",
    "            The learning rate used for updating the weights. Defaults to 0.01.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This method updates the weights of the perceptron for a number of epochs, using the specified learning rate.\n",
    "        It tracks and prints the average loss per epoch to monitor training progress.\n",
    "        \"\"\"\n",
    "        # Start random weights\n",
    "        self.weights = np.random.randn(X.shape[1] + 1)\n",
    "        # Ensure X has bias terms inserted; reshape X to include bias as the first column\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the bias\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(len(X)):\n",
    "                z = np.dot(X[i], self.weights)\n",
    "                predictions = self.activation_function(z)\n",
    "                error = predictions - y[i]\n",
    "                total_loss += np.sum(self.compute_loss(np.array([predictions]), np.array([y[i]])))  # Compute and accumulate loss\n",
    "                self.backward(X[i], error, z, learning_rate)\n",
    "\n",
    "    \n",
    "            average_loss = total_loss / len(X)\n",
    "            self.average_loss.append(average_loss)\n",
    "            print(f'Epoch {epoch + 1}, Average Loss: {average_loss}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output for the given input data using the trained perceptron model.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The input data matrix where each row represents an instance and each column represents a feature.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The predictions generated by the perceptron for each input instance.\n",
    "        \"\"\"\n",
    "        # Add the bias term to the input data\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Insert bias\n",
    "        z = np.dot(X, self.weights)  # Compute the linear combination of inputs and weights\n",
    "        predictions = self.activation_function(z)  # Apply the activation function\n",
    "        return predictions\n",
    "\n",
    "    def predict_classes(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given input data using the trained perceptron model.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The input data array where each row represents an instance.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The class predictions (0 or 1) for each instance, based on a threshold of 0.5.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)  # This should use your existing predict method that outputs probabilities\n",
    "        class_labels = (predictions > 0.5).astype(int)  # Convert probabilities to 0 or 1 based on the threshold\n",
    "        return class_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "y = data.target\n",
    "X = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|\\n',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(dimension_data=4)  # Initialize with one feature\n",
    "perceptron.set_activation_function('tanh')  # Set sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.42985345345315884\n",
      "Epoch 2, Average Loss: 0.6657672956052593\n",
      "Epoch 3, Average Loss: 0.6652332380622165\n",
      "Epoch 4, Average Loss: 0.6647420114678856\n",
      "Epoch 5, Average Loss: 0.6641142172586383\n",
      "Epoch 6, Average Loss: 0.6631033779604701\n",
      "Epoch 7, Average Loss: 0.6608483454108276\n",
      "Epoch 8, Average Loss: 0.6404538028946516\n",
      "Epoch 9, Average Loss: 0.4956273930207642\n",
      "Epoch 10, Average Loss: 0.42770111181437037\n",
      "Epoch 11, Average Loss: 0.3996392763359263\n",
      "Epoch 12, Average Loss: 0.38586848953599895\n",
      "Epoch 13, Average Loss: 0.37803932326942863\n",
      "Epoch 14, Average Loss: 0.3730955294417655\n",
      "Epoch 15, Average Loss: 0.36976887545552617\n",
      "Epoch 16, Average Loss: 0.3674222011053856\n",
      "Epoch 17, Average Loss: 0.3656949535061741\n",
      "Epoch 18, Average Loss: 0.36436989439869594\n",
      "Epoch 19, Average Loss: 0.36331210662181956\n",
      "Epoch 20, Average Loss: 0.3624361375429332\n",
      "Epoch 21, Average Loss: 0.3616868712235303\n",
      "Epoch 22, Average Loss: 0.3610280827560844\n",
      "Epoch 23, Average Loss: 0.3604354994596774\n",
      "Epoch 24, Average Loss: 0.35989254344304455\n",
      "Epoch 25, Average Loss: 0.3593876822643231\n",
      "Epoch 26, Average Loss: 0.35891275348834734\n",
      "Epoch 27, Average Loss: 0.35846188660477285\n",
      "Epoch 28, Average Loss: 0.35803079689047035\n",
      "Epoch 29, Average Loss: 0.3576163146816301\n",
      "Epoch 30, Average Loss: 0.357216066182617\n",
      "Epoch 31, Average Loss: 0.35682825348382424\n",
      "Epoch 32, Average Loss: 0.35645150061112346\n",
      "Epoch 33, Average Loss: 0.35608474422808795\n",
      "Epoch 34, Average Loss: 0.35572715499413526\n",
      "Epoch 35, Average Loss: 0.35537808027267653\n",
      "Epoch 36, Average Loss: 0.3550370019102836\n",
      "Epoch 37, Average Loss: 0.3547035047903639\n",
      "Epoch 38, Average Loss: 0.35437725318202384\n",
      "Epoch 39, Average Loss: 0.354057972792011\n",
      "Epoch 40, Average Loss: 0.3537454370331079\n",
      "Epoch 41, Average Loss: 0.3534394564406828\n",
      "Epoch 42, Average Loss: 0.35313987046157025\n",
      "Epoch 43, Average Loss: 0.3528465410462036\n",
      "Epoch 44, Average Loss: 0.3525593476226119\n",
      "Epoch 45, Average Loss: 0.3522781831374741\n",
      "Epoch 46, Average Loss: 0.35200295092703593\n",
      "Epoch 47, Average Loss: 0.35173356223774405\n",
      "Epoch 48, Average Loss: 0.35146993425872924\n",
      "Epoch 49, Average Loss: 0.35121198855985336\n",
      "Epoch 50, Average Loss: 0.35095964985281813\n"
     ]
    }
   ],
   "source": [
    "perceptron.train(X, y, epochs=50, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.067308668850862,\n",
       " 1.0452320369485901,\n",
       " 1.0237871910375036,\n",
       " 1.003034640598111,\n",
       " 0.9830214870223547,\n",
       " 0.9637814136261573,\n",
       " 0.9453355208263541,\n",
       " 0.927693786233213,\n",
       " 0.9108568922442046,\n",
       " 0.8948181765982491,\n",
       " 0.8795655080248009,\n",
       " 0.8650829505377436,\n",
       " 0.8513521404324426,\n",
       " 0.8383533501396837,\n",
       " 0.8260662491696488,\n",
       " 0.814470394970023,\n",
       " 0.803545498256047,\n",
       " 0.7932715114201401,\n",
       " 0.7836285877990903,\n",
       " 0.7745969559421257,\n",
       " 0.7661567479143407,\n",
       " 0.7582878148336526,\n",
       " 0.7509695566460676,\n",
       " 0.7441807867782454,\n",
       " 0.7378996459072766,\n",
       " 0.7321035728302902,\n",
       " 0.7267693345379292,\n",
       " 0.7218731123765731,\n",
       " 0.7173906369033085,\n",
       " 0.7132973609179462,\n",
       " 0.7095686583187388,\n",
       " 0.7061800358667857,\n",
       " 0.7031073455265775,\n",
       " 0.7003269865440016,\n",
       " 0.6978160885356937,\n",
       " 0.6955526692879475,\n",
       " 0.6935157634196919,\n",
       " 0.6916855203285098,\n",
       " 0.6900432717584886,\n",
       " 0.6885715708227288,\n",
       " 0.687254205363041,\n",
       " 0.6860761891633793,\n",
       " 0.6850237348111474,\n",
       " 0.6840842119958174,\n",
       " 0.6832460948240106,\n",
       " 0.6824989013852665,\n",
       " 0.6818331283830945,\n",
       " 0.6812401831987058,\n",
       " 0.6807123153145505,\n",
       " 0.6802425486143023,\n",
       " 0.679824615709002,\n",
       " 0.679452895121999,\n",
       " 0.6791223518993752,\n",
       " 0.6788284819953984,\n",
       " 0.6785672606098894,\n",
       " 0.6783350945207726,\n",
       " 0.6781287783546999,\n",
       " 0.6779454546659903,\n",
       " 0.6777825776438514,\n",
       " 0.6776378802355397,\n",
       " 0.6775093444546664,\n",
       " 0.6773951746360275,\n",
       " 0.6772937733983677,\n",
       " 0.677203720082193,\n",
       " 0.6771237514393849,\n",
       " 0.6770527443635304,\n",
       " 0.6769897004636235,\n",
       " 0.6769337322982197,\n",
       " 0.6768840511017542,\n",
       " 0.6768399558490908,\n",
       " 0.676800823518211,\n",
       " 0.6767661004240663,\n",
       " 0.6767352945089073,\n",
       " 0.6767079684858038,\n",
       " 0.6766837337425651,\n",
       " 0.6766622449228747,\n",
       " 0.6766431951101862,\n",
       " 0.676626311547863,\n",
       " 0.6766113518361649,\n",
       " 0.6765981005531665,\n",
       " 0.6765863662524236,\n",
       " 0.6765759787954226,\n",
       " 0.6765667869814627,\n",
       " 0.6765586564417599,\n",
       " 0.6765514677682596,\n",
       " 0.6765451148509244,\n",
       " 0.6765395034002037,\n",
       " 0.6765345496339862,\n",
       " 0.6765301791106542,\n",
       " 0.6765263256919323,\n",
       " 0.6765229306210242,\n",
       " 0.6765199417031823,\n",
       " 0.6765173125772833,\n",
       " 0.6765150020682614,\n",
       " 0.6765129736113981,\n",
       " 0.6765111947404612,\n",
       " 0.6765096366326004,\n",
       " 0.6765082737036779,\n",
       " 0.6765070832484377,\n",
       " 0.6765060451205313]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.average_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
