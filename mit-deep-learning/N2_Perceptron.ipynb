{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    A simple Perceptron implementation.\n",
    "\n",
    "    The Perceptron is a fundamental binary classifier that uses a linear decision boundary \n",
    "    to classify data points. This implementation includes basic functionality of a Perceptron \n",
    "    with customizable activation functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dimension_data : int\n",
    "        The number of features (excluding the bias) in the input data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : callable\n",
    "        The function used for activation.\n",
    "    diff_activation_function : callable\n",
    "        The derivative of the activation function.\n",
    "    bias : float\n",
    "        The bias term added to the input data.\n",
    "    weights : np.ndarray\n",
    "        The weights of the perceptron, initialized to None.\n",
    "    activation_function_name : str\n",
    "        The name of the activation function currently in use.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    set_activation_function(name='sigmoid')\n",
    "        Set the activation function by name.\n",
    "    insert_bias(x)\n",
    "        Insert a bias term into the input vector.\n",
    "    loss_function(weights, x, y)\n",
    "        Calculate the loss function given weights, inputs, and the target.\n",
    "    compute_loss(predictions, targets)\n",
    "        Compute the loss between predictions and targets.\n",
    "    forward(X, y, epochs=100, learning_rate=0.01)\n",
    "        Run the forward training loop for the perceptron.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension_data) -> None:\n",
    "        self.activation_function = None\n",
    "        self.diff_activation_function = None\n",
    "        self.bias = 1\n",
    "        self.weights = None\n",
    "        self.activation_function_name = 'sigmoid'\n",
    "    \n",
    "    def set_activation_function(self, name = 'sigmoid'):\n",
    "        \"\"\"\n",
    "        Set the activation function for the perceptron.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str, optional\n",
    "            The name of the activation function. Supported values are 'relu', 'sigmoid', and 'tanh'.\n",
    "            Default is 'sigmoid'.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the specified activation function name is not supported.\n",
    "        \"\"\"\n",
    "        self.activation_function_name = name.lower()\n",
    "        if name.lower() == 'relu':\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "            self.diff_activation_function = lambda x: np.where(x > 0, 1, 0)\n",
    "        elif name.lower() == 'sigmoid':\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.diff_activation_function = lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif name.lower() == 'tanh':\n",
    "            self.activation_function = lambda x: np.tanh(x)\n",
    "            self.diff_activation_function = lambda x: 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "    def insert_bias(self, x):\n",
    "        return np.insert(x, 0, self.bias)  # Always insert bias at index 0\n",
    "    \n",
    "    def loss_function(self, weights, x, y):\n",
    "        prediction = self.activation_function(np.dot(weights, x))\n",
    "        return prediction - y\n",
    "\n",
    "    def compute_loss(self, predictions, targets):\n",
    "        # Check which activation function is used based on a stored name attribute\n",
    "        if self.activation_function_name == 'sigmoid':\n",
    "            # Binary cross-entropy for sigmoid activation\n",
    "            return -np.mean(targets * np.log(predictions + 1e-9) + (1 - targets) * np.log(1 - predictions + 1e-9))\n",
    "        elif self.activation_function_name == 'relu' or self.activation_function_name == 'tanh':\n",
    "            # Mean squared error for ReLU or tanh activations in a regression context\n",
    "            return np.mean((predictions - targets) ** 2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported or undefined activation function name: {self.activation_function_name}\")\n",
    "\n",
    "    \n",
    "                \n",
    "    def forward(self, X, y, epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Perform the forward pass and update weights based on the training data.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            The input data matrix.\n",
    "        y : np.ndarray\n",
    "            The target output vector.\n",
    "        epochs : int, optional\n",
    "            The number of epochs to train the model. Default is 100.\n",
    "        learning_rate : float, optional\n",
    "            The learning rate for weight updates. Default is 0.01.\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        This method updates the weights based on the loss gradient and prints the average loss per epoch.\n",
    "        \"\"\"\n",
    "        # Start random weights\n",
    "        self.weights = np.random.randn(X.shape[1] + 1)\n",
    "        # Ensure X has bias terms inserted; reshape X to include bias as the first column\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the bias\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(len(X)):\n",
    "                z = np.dot(X[i], self.weights)  # Calculate the linear combination for each instance\n",
    "                predictions = self.activation_function(z)\n",
    "                loss = self.compute_loss(np.array([predictions]), np.array([y[i]]))  # Compute loss for the current instance\n",
    "                total_loss += loss\n",
    "                error = predictions - y[i]  # Calculate error for gradient\n",
    "                gradient = error * self.diff_activation_function(z) * X[i]\n",
    "                self.weights -= learning_rate * gradient  # Properly adjust sign for weight update\n",
    "    \n",
    "            average_loss = total_loss / len(X)\n",
    "            print(f'Epoch {epoch + 1}, Average Loss: {average_loss}')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data points and random binary targets\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X = np.random.randn(100, 1)  # 100 unidimensional data points\n",
    "y = np.random.randint(0, 2, size=(100,))  # Binary targets (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(dimension_data=1)  # Initialize with one feature\n",
    "perceptron.set_activation_function('sigmoid')  # Set sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.8228163422351933\n",
      "Epoch 2, Average Loss: 0.8128472872298872\n",
      "Epoch 3, Average Loss: 0.8033136807224527\n",
      "Epoch 4, Average Loss: 0.7942128136727276\n",
      "Epoch 5, Average Loss: 0.7855423328295688\n",
      "Epoch 6, Average Loss: 0.7772999248762433\n",
      "Epoch 7, Average Loss: 0.7694830028888946\n",
      "Epoch 8, Average Loss: 0.7620884058458808\n",
      "Epoch 9, Average Loss: 0.755112122284088\n",
      "Epoch 10, Average Loss: 0.7485490490350903\n",
      "Epoch 11, Average Loss: 0.74239279511012\n",
      "Epoch 12, Average Loss: 0.7366355391494587\n",
      "Epoch 13, Average Loss: 0.7312679464319715\n",
      "Epoch 14, Average Loss: 0.7262791483952885\n",
      "Epoch 15, Average Loss: 0.7216567841937461\n",
      "Epoch 16, Average Loss: 0.7173871003396975\n",
      "Epoch 17, Average Loss: 0.7134551012799834\n",
      "Epoch 18, Average Loss: 0.7098447411696731\n",
      "Epoch 19, Average Loss: 0.706539145357897\n",
      "Epoch 20, Average Loss: 0.7035208493211671\n",
      "Epoch 21, Average Loss: 0.7007720429685526\n",
      "Epoch 22, Average Loss: 0.6982748092864469\n",
      "Epoch 23, Average Loss: 0.6960113479892187\n",
      "Epoch 24, Average Loss: 0.6939641769505126\n",
      "Epoch 25, Average Loss: 0.6921163064579339\n",
      "Epoch 26, Average Loss: 0.6904513835394139\n",
      "Epoch 27, Average Loss: 0.6889538055819032\n",
      "Epoch 28, Average Loss: 0.6876088040921441\n",
      "Epoch 29, Average Loss: 0.686402500684244\n",
      "Epoch 30, Average Loss: 0.6853219382186531\n",
      "Epoch 31, Average Loss: 0.6843550904973299\n",
      "Epoch 32, Average Loss: 0.683490854097994\n",
      "Epoch 33, Average Loss: 0.6827190258736094\n",
      "Epoch 34, Average Loss: 0.6820302694187632\n",
      "Epoch 35, Average Loss: 0.6814160734729171\n",
      "Epoch 36, Average Loss: 0.6808687048420903\n",
      "Epoch 37, Average Loss: 0.6803811580143695\n",
      "Epoch 38, Average Loss: 0.6799471032489929\n",
      "Epoch 39, Average Loss: 0.6795608345520543\n",
      "Epoch 40, Average Loss: 0.6792172186249488\n",
      "Epoch 41, Average Loss: 0.6789116455892673\n",
      "Epoch 42, Average Loss: 0.6786399820545245\n",
      "Epoch 43, Average Loss: 0.6783985269005727\n",
      "Epoch 44, Average Loss: 0.6781839699909478\n",
      "Epoch 45, Average Loss: 0.6779933539118104\n",
      "Epoch 46, Average Loss: 0.6778240387387576\n",
      "Epoch 47, Average Loss: 0.6776736697655497\n",
      "Epoch 48, Average Loss: 0.6775401480804232\n",
      "Epoch 49, Average Loss: 0.6774216038430335\n",
      "Epoch 50, Average Loss: 0.6773163720949281\n",
      "Epoch 51, Average Loss: 0.6772229709257767\n",
      "Epoch 52, Average Loss: 0.6771400818141443\n",
      "Epoch 53, Average Loss: 0.6770665319633737\n",
      "Epoch 54, Average Loss: 0.6770012784586786\n",
      "Epoch 55, Average Loss: 0.6769433940796022\n",
      "Epoch 56, Average Loss: 0.6768920546116634\n",
      "Epoch 57, Average Loss: 0.6768465275115987\n",
      "Epoch 58, Average Loss: 0.676806161791546\n",
      "Epoch 59, Average Loss: 0.6767703789984865\n",
      "Epoch 60, Average Loss: 0.6767386651759556\n",
      "Epoch 61, Average Loss: 0.6767105637052592\n",
      "Epoch 62, Average Loss: 0.676685668933108\n",
      "Epoch 63, Average Loss: 0.6766636205016283\n",
      "Epoch 64, Average Loss: 0.6766440983050456\n",
      "Epoch 65, Average Loss: 0.6766268180050481\n",
      "Epoch 66, Average Loss: 0.676611527043856\n",
      "Epoch 67, Average Loss: 0.6765980011004109\n",
      "Epoch 68, Average Loss: 0.6765860409409042\n",
      "Epoch 69, Average Loss: 0.6765754696200674\n",
      "Epoch 70, Average Loss: 0.6765661299943909\n",
      "Epoch 71, Average Loss: 0.6765578825126167\n",
      "Epoch 72, Average Loss: 0.6765506032526989\n",
      "Epoch 73, Average Loss: 0.6765441821777565\n",
      "Epoch 74, Average Loss: 0.6765385215866284\n",
      "Epoch 75, Average Loss: 0.6765335347373038\n",
      "Epoch 76, Average Loss: 0.676529144623933\n",
      "Epoch 77, Average Loss: 0.6765252828902745\n",
      "Epoch 78, Average Loss: 0.6765218888643291\n",
      "Epoch 79, Average Loss: 0.6765189087006295\n",
      "Epoch 80, Average Loss: 0.6765162946181618\n",
      "Epoch 81, Average Loss: 0.6765140042232354\n",
      "Epoch 82, Average Loss: 0.6765119999078173\n",
      "Epoch 83, Average Loss: 0.6765102483149091\n",
      "Epoch 84, Average Loss: 0.6765087198634796\n",
      "Epoch 85, Average Loss: 0.6765073883263204\n",
      "Epoch 86, Average Loss: 0.6765062304549093\n",
      "Epoch 87, Average Loss: 0.6765052256460539\n",
      "Epoch 88, Average Loss: 0.6765043556456537\n",
      "Epoch 89, Average Loss: 0.6765036042854484\n",
      "Epoch 90, Average Loss: 0.6765029572490794\n",
      "Epoch 91, Average Loss: 0.6765024018642021\n",
      "Epoch 92, Average Loss: 0.6765019269177536\n",
      "Epoch 93, Average Loss: 0.676501522491795\n",
      "Epoch 94, Average Loss: 0.6765011798176461\n",
      "Epoch 95, Average Loss: 0.6765008911462751\n",
      "Epoch 96, Average Loss: 0.6765006496331396\n",
      "Epoch 97, Average Loss: 0.6765004492358732\n",
      "Epoch 98, Average Loss: 0.6765002846233866\n",
      "Epoch 99, Average Loss: 0.676500151095123\n",
      "Epoch 100, Average Loss: 0.6765000445093297\n"
     ]
    }
   ],
   "source": [
    "perceptron.forward(X, y, epochs=100, learning_rate=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
